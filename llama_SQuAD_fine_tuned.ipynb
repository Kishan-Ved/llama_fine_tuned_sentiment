{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Downloads","metadata":{}},{"cell_type":"code","source":"!pip install kagglehub transformers datasets evaluate prettytable accelerate nltk\n!pip install --update nltk\n\n# Clear the Out output of the cell\nfrom IPython.display import clear_output\nclear_output()\n\nprint(\"Downloaded required Libraries\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:07:36.262022Z","iopub.execute_input":"2024-11-23T04:07:36.262346Z","iopub.status.idle":"2024-11-23T04:07:47.914817Z","shell.execute_reply.started":"2024-11-23T04:07:36.262317Z","shell.execute_reply":"2024-11-23T04:07:47.913744Z"}},"outputs":[{"name":"stdout","text":"Downloaded required Libraries\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport nltk\nimport torch\nimport random\nimport shutil\nimport evaluate\nimport kagglehub\nimport numpy as np\nimport pandas as pd\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nfrom evaluate import load\nfrom transformers import pipeline\nfrom prettytable import PrettyTable\nfrom huggingface_hub import create_repo\nfrom IPython.display import display, HTML\nfrom huggingface_hub import upload_folder\nfrom accelerate import infer_auto_device_map\nfrom datasets import load_dataset, ClassLabel, Sequence\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoModelForQuestionAnswering, Trainer, TrainingArguments, BitsAndBytesConfig, default_data_collator, TrainerCallback\n\nclear_output()\nprint(\"Imported All Required Libraries\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:21:41.543460Z","iopub.execute_input":"2024-11-23T03:21:41.544341Z","iopub.status.idle":"2024-11-23T03:21:41.552205Z","shell.execute_reply.started":"2024-11-23T03:21:41.544301Z","shell.execute_reply":"2024-11-23T03:21:41.551355Z"}},"outputs":[{"name":"stdout","text":"Imported All Required Libraries\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## Login","metadata":{}},{"cell_type":"code","source":"!huggingface-cli login --token=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","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:21:50.618379Z","iopub.execute_input":"2024-11-23T03:21:50.618746Z","iopub.status.idle":"2024-11-23T03:21:52.287096Z","shell.execute_reply.started":"2024-11-23T03:21:50.618708Z","shell.execute_reply":"2024-11-23T03:21:52.286189Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"## Load Datasets","metadata":{}},{"cell_type":"code","source":"datasets = load_dataset(\"rajpurkar/squad_v2\")\nprint(datasets)\nprint(datasets['train'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:22:07.824679Z","iopub.execute_input":"2024-11-23T03:22:07.825103Z","iopub.status.idle":"2024-11-23T03:22:09.681259Z","shell.execute_reply.started":"2024-11-23T03:22:07.825065Z","shell.execute_reply":"2024-11-23T03:22:09.680431Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 130319\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 11873\n    })\n})\n{'id': '56be85543aeaaa14008c9063', 'title': 'Beyoncé', 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?', 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"def show_random_elements(dataset, num_examples=5):\n    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n    picks = []\n    for _ in range(num_examples):\n        pick = random.randint(0, len(dataset)-1)\n        while pick in picks:\n            pick = random.randint(0, len(dataset)-1)\n        picks.append(pick)\n    \n    df = pd.DataFrame(dataset[picks])\n    for column, typ in dataset.features.items():\n        if isinstance(typ, ClassLabel):\n            df[column] = df[column].transform(lambda i: typ.names[i])\n        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n    display(HTML(df.to_html()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:22:17.079174Z","iopub.execute_input":"2024-11-23T03:22:17.079895Z","iopub.status.idle":"2024-11-23T03:22:17.086596Z","shell.execute_reply.started":"2024-11-23T03:22:17.079859Z","shell.execute_reply":"2024-11-23T03:22:17.085626Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"show_random_elements(datasets[\"train\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:22:19.580343Z","iopub.execute_input":"2024-11-23T03:22:19.580732Z","iopub.status.idle":"2024-11-23T03:22:19.605860Z","shell.execute_reply.started":"2024-11-23T03:22:19.580696Z","shell.execute_reply":"2024-11-23T03:22:19.604795Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>context</th>\n      <th>question</th>\n      <th>answers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5727ebe03acd2414000deff0</td>\n      <td>Gamal_Abdel_Nasser</td>\n      <td>Nasser remains an iconic figure in the Arab world, particularly for his strides towards social justice and Arab unity, modernization policies, and anti-imperialist efforts. His presidency also encouraged and coincided with an Egyptian cultural boom, and launched large industrial projects, including the Aswan Dam and Helwan City. Nasser's detractors criticize his authoritarianism, his government's human rights violations, his populist relationship with the citizenry, and his failure to establish civil institutions, blaming his legacy for future dictatorial governance in Egypt. Historians describe Nasser as a towering political figure of the Middle East in the 20th century.</td>\n      <td>What century did Nasser rule in?</td>\n      <td>{'text': ['20th'], 'answer_start': [667]}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>56dff532231d4119001abf05</td>\n      <td>Pub</td>\n      <td>In Ireland, pubs are known for their atmosphere or \"craic\". In Irish, a pub is referred to as teach tábhairne (\"tavernhouse\") or teach óil (\"drinkinghouse\"). Live music, either sessions of traditional Irish music or varieties of modern popular music, is frequently featured in the pubs of Ireland. Pubs in Northern Ireland are largely identical to their counterparts in the Republic of Ireland except for the lack of spirit grocers. A side effect of \"The Troubles\" was that the lack of a tourist industry meant that a higher proportion of traditional bars have survived the wholesale refitting of Irish pub interiors in the 'English style' in the 1950s and 1960s. New Zealand sports a number of Irish pubs.</td>\n      <td>What country outside Ireland is known for having Irish pubs?</td>\n      <td>{'text': ['New Zealand'], 'answer_start': [664]}</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>56d61c4e1c85041400946f1d</td>\n      <td>2008_Sichuan_earthquake</td>\n      <td>The Internet was extensively used for passing information to aid rescue and recovery efforts. For example, the official news agency Xinhua set up an online rescue request center in order to find the blind spots of disaster recovery. After knowing that rescue helicopters had trouble landing into the epicenter area in Wenchuan, a student proposed a landing spot online and it was chosen as the first touchdown place for the helicopters[not in citation given]. Volunteers also set up several websites to help store contact information for victims and evacuees. On May 31, a rescue helicopter carrying earthquake survivors and crew members crashed in fog and turbulence in Wenchuan county. No-one survived.</td>\n      <td>What kind of information were websites set up to store?</td>\n      <td>{'text': ['contact information'], 'answer_start': [514]}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>572abec0be1ee31400cb8201</td>\n      <td>Friedrich_Hayek</td>\n      <td>In Why F A Hayek is a Conservative, British policy analyst Madsen Pirie claims Hayek mistakes the nature of the conservative outlook. Conservatives, he says, are not averse to change – but like Hayek, they are highly averse to change being imposed on the social order by people in authority who think they know how to run things better. They wish to allow the market to function smoothly and give it the freedom to change and develop. It is an outlook, says Pirie, that Hayek and conservatives both share.</td>\n      <td>Pirie believes Hayek to be a conservative for what reason?</td>\n      <td>{'text': ['mistakes the nature of the conservative outlook'], 'answer_start': [85]}</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5a36f18395360f001af1b36c</td>\n      <td>Gregorian_calendar</td>\n      <td>To unambiguously specify the date, dual dating or Old Style (O.S.) and New Style (N.S.) are sometimes used with dates. Dual dating uses two consecutive years because of differences in the starting date of the year, or includes both the Julian and Gregorian dates. Old Style and New Style (N.S.) indicate either whether the start of the Julian year has been adjusted to start on 1 January (N.S.) even though documents written at the time use a different start of year (O.S.), or whether a date conforms to the Julian calendar (O.S.) rather than the Gregorian (N.S.).</td>\n      <td>Which system indicates that the date that the Julian date has been adjusted for length?</td>\n      <td>{'text': [], 'answer_start': []}</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"## Defining Variables","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"meta-llama/Llama-3.2-1B\"\nhf_token = \"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\"\n\nmax_length = 384 # The maximum length of a feature (question and context)\ndoc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\npad_on_right = tokenizer.padding_side == \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:28:11.758206Z","iopub.execute_input":"2024-11-22T22:28:11.758464Z","iopub.status.idle":"2024-11-22T22:28:11.762466Z","shell.execute_reply.started":"2024-11-22T22:28:11.758438Z","shell.execute_reply":"2024-11-22T22:28:11.761667Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, token = hf_token)\n\nclear_output()\nprint(\"Tokenizer Loaded Successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:28:11.763436Z","iopub.execute_input":"2024-11-22T22:28:11.763703Z","iopub.status.idle":"2024-11-22T22:28:13.505685Z","shell.execute_reply.started":"2024-11-22T22:28:11.763678Z","shell.execute_reply":"2024-11-22T22:28:13.504558Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b90842447cd44190b11a927183e0b128"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9918efed40a24ad89d3ea7e2e9920987"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6b3bb8a1bf24e518c98a4083c379da4"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"## Credits: https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb\n\ndef prepare_train_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # To keep track of which example each split belongs to\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Initialize start and end positions\n    start_positions = []\n    end_positions = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # Map this tokenized example back to the original example\n        sample_index = sample_mapping[i]\n        answer_start_list = examples[\"answers\"][sample_index][\"answer_start\"]\n        answer_text_list = examples[\"answers\"][sample_index][\"text\"]\n\n        # If no answer is available, set positions to the fallback index\n        if len(answer_start_list) == 0:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Compute character-level start and end positions\n            start_char = answer_start_list[0]\n            end_char = start_char + len(answer_text_list[0])\n\n            # Map character positions to token positions\n            token_start = None\n            token_end = None\n            for idx, (start, end) in enumerate(offsets):\n                if start <= start_char < end:\n                    token_start = idx\n                if start < end_char <= end:\n                    token_end = idx\n                    break\n\n            if token_start is None or token_end is None:\n                # Fallback if mapping fails\n                start_positions.append(0)\n                end_positions.append(0)\n            else:\n                start_positions.append(token_start)\n                end_positions.append(token_end)\n\n    # Add start and end positions to the tokenized examples\n    tokenized_examples[\"start_positions\"] = start_positions\n    tokenized_examples[\"end_positions\"] = end_positions\n    return tokenized_examples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:28:13.514891Z","iopub.execute_input":"2024-11-22T22:28:13.515145Z","iopub.status.idle":"2024-11-22T22:28:13.530644Z","shell.execute_reply.started":"2024-11-22T22:28:13.515120Z","shell.execute_reply":"2024-11-22T22:28:13.529834Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"if tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ntokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)\n\nclear_output()\nprint(\"Data tokenized Successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:28:13.531530Z","iopub.execute_input":"2024-11-22T22:28:13.531941Z","iopub.status.idle":"2024-11-22T22:28:13.544263Z","shell.execute_reply.started":"2024-11-22T22:28:13.531901Z","shell.execute_reply":"2024-11-22T22:28:13.543598Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Define Matric","metadata":{}},{"cell_type":"code","source":"# Load accuracy metric\nmetric = evaluate.load(\"accuracy\")\n\n# Define compute_metrics function\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:29:13.694771Z","iopub.execute_input":"2024-11-22T22:29:13.695045Z","iopub.status.idle":"2024-11-22T22:29:14.176516Z","shell.execute_reply.started":"2024-11-22T22:29:13.695017Z","shell.execute_reply":"2024-11-22T22:29:14.175919Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9e3b0969ea74548bbda3c6548eb3b50"}},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"## Loading Model","metadata":{}},{"cell_type":"code","source":"model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n\nclear_output()\nprint(\"Model Loaded Successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:29:14.177440Z","iopub.execute_input":"2024-11-22T22:29:14.177686Z","iopub.status.idle":"2024-11-22T22:30:26.268861Z","shell.execute_reply.started":"2024-11-22T22:29:14.177660Z","shell.execute_reply":"2024-11-22T22:30:26.267984Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"599510709bab426c9f05783c44f6b5b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e247c26f6c5142ad8d81b6be7e07b0b1"}},"metadata":{}},{"name":"stderr","text":"Some weights of LlamaForQuestionAnswering were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'norm.weight', 'qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Parameter count and freezing","metadata":{}},{"cell_type":"code","source":"def count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if not parameter.requires_grad:\n            continue\n        params = parameter.numel()\n        table.add_row([name, params])\n        total_params += params\n    print(table)\n    print(f\"Total Trainable Params: {total_params}\")\n    return total_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:30:26.270135Z","iopub.execute_input":"2024-11-22T22:30:26.270531Z","iopub.status.idle":"2024-11-22T22:30:26.275753Z","shell.execute_reply.started":"2024-11-22T22:30:26.270489Z","shell.execute_reply":"2024-11-22T22:30:26.274879Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"_ = count_parameters(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:30:26.276805Z","iopub.execute_input":"2024-11-22T22:30:26.277060Z","iopub.status.idle":"2024-11-22T22:30:29.146522Z","shell.execute_reply.started":"2024-11-22T22:30:26.277034Z","shell.execute_reply":"2024-11-22T22:30:29.145653Z"}},"outputs":[{"name":"stdout","text":"+-------------------------------------------------------+------------+\n|                        Modules                        | Parameters |\n+-------------------------------------------------------+------------+\n|            transformer.embed_tokens.weight            | 262668288  |\n|      transformer.layers.0.self_attn.q_proj.weight     |  4194304   |\n|      transformer.layers.0.self_attn.k_proj.weight     |  1048576   |\n|      transformer.layers.0.self_attn.v_proj.weight     |  1048576   |\n|      transformer.layers.0.self_attn.o_proj.weight     |  4194304   |\n|       transformer.layers.0.mlp.gate_proj.weight       |  16777216  |\n|        transformer.layers.0.mlp.up_proj.weight        |  16777216  |\n|       transformer.layers.0.mlp.down_proj.weight       |  16777216  |\n|      transformer.layers.0.input_layernorm.weight      |    2048    |\n|  transformer.layers.0.post_attention_layernorm.weight |    2048    |\n|      transformer.layers.1.self_attn.q_proj.weight     |  4194304   |\n|      transformer.layers.1.self_attn.k_proj.weight     |  1048576   |\n|      transformer.layers.1.self_attn.v_proj.weight     |  1048576   |\n|      transformer.layers.1.self_attn.o_proj.weight     |  4194304   |\n|       transformer.layers.1.mlp.gate_proj.weight       |  16777216  |\n|        transformer.layers.1.mlp.up_proj.weight        |  16777216  |\n|       transformer.layers.1.mlp.down_proj.weight       |  16777216  |\n|      transformer.layers.1.input_layernorm.weight      |    2048    |\n|  transformer.layers.1.post_attention_layernorm.weight |    2048    |\n|      transformer.layers.2.self_attn.q_proj.weight     |  4194304   |\n|      transformer.layers.2.self_attn.k_proj.weight     |  1048576   |\n|      transformer.layers.2.self_attn.v_proj.weight     |  1048576   |\n|      transformer.layers.2.self_attn.o_proj.weight     |  4194304   |\n|       transformer.layers.2.mlp.gate_proj.weight       |  16777216  |\n|        transformer.layers.2.mlp.up_proj.weight        |  16777216  |\n|       transformer.layers.2.mlp.down_proj.weight       |  16777216  |\n|      transformer.layers.2.input_layernorm.weight      |    2048    |\n|  transformer.layers.2.post_attention_layernorm.weight |    2048    |\n|      transformer.layers.3.self_attn.q_proj.weight     |  4194304   |\n|      transformer.layers.3.self_attn.k_proj.weight     |  1048576   |\n|      transformer.layers.3.self_attn.v_proj.weight     |  1048576   |\n|      transformer.layers.3.self_attn.o_proj.weight     |  4194304   |\n|       transformer.layers.3.mlp.gate_proj.weight       |  16777216  |\n|        transformer.layers.3.mlp.up_proj.weight        |  16777216  |\n|       transformer.layers.3.mlp.down_proj.weight       |  16777216  |\n|      transformer.layers.3.input_layernorm.weight      |    2048    |\n|  transformer.layers.3.post_attention_layernorm.weight |    2048    |\n|      transformer.layers.4.self_attn.q_proj.weight     |  4194304   |\n|      transformer.layers.4.self_attn.k_proj.weight     |  1048576   |\n|      transformer.layers.4.self_attn.v_proj.weight     |  1048576   |\n|      transformer.layers.4.self_attn.o_proj.weight     |  4194304   |\n|       transformer.layers.4.mlp.gate_proj.weight       |  16777216  |\n|        transformer.layers.4.mlp.up_proj.weight        |  16777216  |\n|       transformer.layers.4.mlp.down_proj.weight       |  16777216  |\n|      transformer.layers.4.input_layernorm.weight      |    2048    |\n|  transformer.layers.4.post_attention_layernorm.weight |    2048    |\n|      transformer.layers.5.self_attn.q_proj.weight     |  4194304   |\n|      transformer.layers.5.self_attn.k_proj.weight     |  1048576   |\n|      transformer.layers.5.self_attn.v_proj.weight     |  1048576   |\n|      transformer.layers.5.self_attn.o_proj.weight     |  4194304   |\n|       transformer.layers.5.mlp.gate_proj.weight       |  16777216  |\n|        transformer.layers.5.mlp.up_proj.weight        |  16777216  |\n|       transformer.layers.5.mlp.down_proj.weight       |  16777216  |\n|      transformer.layers.5.input_layernorm.weight      |    2048    |\n|  transformer.layers.5.post_attention_layernorm.weight |    2048    |\n|      transformer.layers.6.self_attn.q_proj.weight     |  4194304   |\n|      transformer.layers.6.self_attn.k_proj.weight     |  1048576   |\n|      transformer.layers.6.self_attn.v_proj.weight     |  1048576   |\n|      transformer.layers.6.self_attn.o_proj.weight     |  4194304   |\n|       transformer.layers.6.mlp.gate_proj.weight       |  16777216  |\n|        transformer.layers.6.mlp.up_proj.weight        |  16777216  |\n|       transformer.layers.6.mlp.down_proj.weight       |  16777216  |\n|      transformer.layers.6.input_layernorm.weight      |    2048    |\n|  transformer.layers.6.post_attention_layernorm.weight |    2048    |\n|      transformer.layers.7.self_attn.q_proj.weight     |  4194304   |\n|      transformer.layers.7.self_attn.k_proj.weight     |  1048576   |\n|      transformer.layers.7.self_attn.v_proj.weight     |  1048576   |\n|      transformer.layers.7.self_attn.o_proj.weight     |  4194304   |\n|       transformer.layers.7.mlp.gate_proj.weight       |  16777216  |\n|        transformer.layers.7.mlp.up_proj.weight        |  16777216  |\n|       transformer.layers.7.mlp.down_proj.weight       |  16777216  |\n|      transformer.layers.7.input_layernorm.weight      |    2048    |\n|  transformer.layers.7.post_attention_layernorm.weight |    2048    |\n|      transformer.layers.8.self_attn.q_proj.weight     |  4194304   |\n|      transformer.layers.8.self_attn.k_proj.weight     |  1048576   |\n|      transformer.layers.8.self_attn.v_proj.weight     |  1048576   |\n|      transformer.layers.8.self_attn.o_proj.weight     |  4194304   |\n|       transformer.layers.8.mlp.gate_proj.weight       |  16777216  |\n|        transformer.layers.8.mlp.up_proj.weight        |  16777216  |\n|       transformer.layers.8.mlp.down_proj.weight       |  16777216  |\n|      transformer.layers.8.input_layernorm.weight      |    2048    |\n|  transformer.layers.8.post_attention_layernorm.weight |    2048    |\n|      transformer.layers.9.self_attn.q_proj.weight     |  4194304   |\n|      transformer.layers.9.self_attn.k_proj.weight     |  1048576   |\n|      transformer.layers.9.self_attn.v_proj.weight     |  1048576   |\n|      transformer.layers.9.self_attn.o_proj.weight     |  4194304   |\n|       transformer.layers.9.mlp.gate_proj.weight       |  16777216  |\n|        transformer.layers.9.mlp.up_proj.weight        |  16777216  |\n|       transformer.layers.9.mlp.down_proj.weight       |  16777216  |\n|      transformer.layers.9.input_layernorm.weight      |    2048    |\n|  transformer.layers.9.post_attention_layernorm.weight |    2048    |\n|     transformer.layers.10.self_attn.q_proj.weight     |  4194304   |\n|     transformer.layers.10.self_attn.k_proj.weight     |  1048576   |\n|     transformer.layers.10.self_attn.v_proj.weight     |  1048576   |\n|     transformer.layers.10.self_attn.o_proj.weight     |  4194304   |\n|       transformer.layers.10.mlp.gate_proj.weight      |  16777216  |\n|        transformer.layers.10.mlp.up_proj.weight       |  16777216  |\n|       transformer.layers.10.mlp.down_proj.weight      |  16777216  |\n|      transformer.layers.10.input_layernorm.weight     |    2048    |\n| transformer.layers.10.post_attention_layernorm.weight |    2048    |\n|     transformer.layers.11.self_attn.q_proj.weight     |  4194304   |\n|     transformer.layers.11.self_attn.k_proj.weight     |  1048576   |\n|     transformer.layers.11.self_attn.v_proj.weight     |  1048576   |\n|     transformer.layers.11.self_attn.o_proj.weight     |  4194304   |\n|       transformer.layers.11.mlp.gate_proj.weight      |  16777216  |\n|        transformer.layers.11.mlp.up_proj.weight       |  16777216  |\n|       transformer.layers.11.mlp.down_proj.weight      |  16777216  |\n|      transformer.layers.11.input_layernorm.weight     |    2048    |\n| transformer.layers.11.post_attention_layernorm.weight |    2048    |\n|     transformer.layers.12.self_attn.q_proj.weight     |  4194304   |\n|     transformer.layers.12.self_attn.k_proj.weight     |  1048576   |\n|     transformer.layers.12.self_attn.v_proj.weight     |  1048576   |\n|     transformer.layers.12.self_attn.o_proj.weight     |  4194304   |\n|       transformer.layers.12.mlp.gate_proj.weight      |  16777216  |\n|        transformer.layers.12.mlp.up_proj.weight       |  16777216  |\n|       transformer.layers.12.mlp.down_proj.weight      |  16777216  |\n|      transformer.layers.12.input_layernorm.weight     |    2048    |\n| transformer.layers.12.post_attention_layernorm.weight |    2048    |\n|     transformer.layers.13.self_attn.q_proj.weight     |  4194304   |\n|     transformer.layers.13.self_attn.k_proj.weight     |  1048576   |\n|     transformer.layers.13.self_attn.v_proj.weight     |  1048576   |\n|     transformer.layers.13.self_attn.o_proj.weight     |  4194304   |\n|       transformer.layers.13.mlp.gate_proj.weight      |  16777216  |\n|        transformer.layers.13.mlp.up_proj.weight       |  16777216  |\n|       transformer.layers.13.mlp.down_proj.weight      |  16777216  |\n|      transformer.layers.13.input_layernorm.weight     |    2048    |\n| transformer.layers.13.post_attention_layernorm.weight |    2048    |\n|     transformer.layers.14.self_attn.q_proj.weight     |  4194304   |\n|     transformer.layers.14.self_attn.k_proj.weight     |  1048576   |\n|     transformer.layers.14.self_attn.v_proj.weight     |  1048576   |\n|     transformer.layers.14.self_attn.o_proj.weight     |  4194304   |\n|       transformer.layers.14.mlp.gate_proj.weight      |  16777216  |\n|        transformer.layers.14.mlp.up_proj.weight       |  16777216  |\n|       transformer.layers.14.mlp.down_proj.weight      |  16777216  |\n|      transformer.layers.14.input_layernorm.weight     |    2048    |\n| transformer.layers.14.post_attention_layernorm.weight |    2048    |\n|     transformer.layers.15.self_attn.q_proj.weight     |  4194304   |\n|     transformer.layers.15.self_attn.k_proj.weight     |  1048576   |\n|     transformer.layers.15.self_attn.v_proj.weight     |  1048576   |\n|     transformer.layers.15.self_attn.o_proj.weight     |  4194304   |\n|       transformer.layers.15.mlp.gate_proj.weight      |  16777216  |\n|        transformer.layers.15.mlp.up_proj.weight       |  16777216  |\n|       transformer.layers.15.mlp.down_proj.weight      |  16777216  |\n|      transformer.layers.15.input_layernorm.weight     |    2048    |\n| transformer.layers.15.post_attention_layernorm.weight |    2048    |\n|                transformer.norm.weight                |    2048    |\n|                   qa_outputs.weight                   |    4096    |\n|                    qa_outputs.bias                    |     2      |\n+-------------------------------------------------------+------------+\nTotal Trainable Params: 1235818498\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"1235818498"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Freeze all layers except for the question-answering head\nfor name, param in model.named_parameters():\n    if \"qa_outputs\" not in name:  # Unfreeze only the QA head\n        param.requires_grad = False\n\n_ = count_parameters(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:30:29.147686Z","iopub.execute_input":"2024-11-22T22:30:29.147976Z","iopub.status.idle":"2024-11-22T22:30:29.157260Z","shell.execute_reply.started":"2024-11-22T22:30:29.147947Z","shell.execute_reply":"2024-11-22T22:30:29.156350Z"}},"outputs":[{"name":"stdout","text":"+-------------------+------------+\n|      Modules      | Parameters |\n+-------------------+------------+\n| qa_outputs.weight |    4096    |\n|  qa_outputs.bias  |     2      |\n+-------------------+------------+\nTotal Trainable Params: 4098\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"4098"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## Fine Tuning","metadata":{}},{"cell_type":"code","source":"# Custom Logging Callback\nclass CustomLoggingCallback(TrainerCallback):\n    def __init__(self, log_interval=200):\n        super().__init__()\n        self.log_interval = log_interval\n\n    def on_step_end(self, args, state, control, **kwargs):\n        # Log metrics every specified number of steps\n        if state.global_step % self.log_interval == 0:\n            print(f\"Logging metrics at step {state.global_step}:\")\n            if state.log_history:  # Ensure log history is not empty\n                print(f\"Loss: {state.log_history[-1].get('loss', 'N/A')}\")\n                print(f\"Learning rate: {state.log_history[-1].get('learning_rate', 'N/A')}\")\n                print(f\"Metrics: {state.log_history[-1]}\")\n\n\nclass ModelSaveCallback(TrainerCallback):\n    def __init__(self, save_interval_steps=250, save_total_limit=2):\n        super().__init__()\n        self.save_interval_steps = save_interval_steps\n        self.save_total_limit = save_total_limit\n        self.saved_checkpoints = []  # Track saved checkpoints\n\n    def on_step_end(self, args, state, control, **kwargs):\n        if state.global_step % self.save_interval_steps == 0 and state.global_step > 0:\n            model_save_path = f\"{args.output_dir}/model_step_{state.global_step}\"\n            # Save the model\n            kwargs['model'].save_pretrained(model_save_path)\n            print(f\"Model saved at step {state.global_step}: {model_save_path}\")\n\n            # Add the new checkpoint to the list\n            self.saved_checkpoints.append(model_save_path)\n\n            # If saved checkpoints exceed the limit, delete the oldest one\n            if len(self.saved_checkpoints) > self.save_total_limit:\n                oldest_checkpoint = self.saved_checkpoints.pop(0)\n                try:\n                    shutil.rmtree(oldest_checkpoint)  # Remove the checkpoint directory\n                    print(f\"Deleted oldest checkpoint: {oldest_checkpoint}\")\n                except Exception as e:\n                    print(f\"Error deleting checkpoint {oldest_checkpoint}: {e}\")\n\n# Adjusted Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"kaggle_model_results\",\n    evaluation_strategy=\"steps\",\n    eval_steps=2000,  # Less frequent evaluations to save time\n    learning_rate=5e-5,  # Higher learning rate for faster convergence\n    per_device_train_batch_size=64,  # Increase batch size to speed up training\n    per_device_eval_batch_size=32,\n    gradient_accumulation_steps=1,  # No accumulation for simplicity\n    num_train_epochs=2,  # Fewer epochs to speed up\n    weight_decay=0.01,\n    save_steps=0,  # Disable default step-based saving\n    logging_steps=200,  # Log every 20 steps\n    seed=42,\n    fp16=True,\n    report_to=\"none\",\n    dataloader_num_workers=4,\n    save_total_limit=2,  # Keep only the last 2 checkpoints\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=default_data_collator,\n    tokenizer=tokenizer,\n    callbacks=[\n        CustomLoggingCallback(log_interval=200),\n        ModelSaveCallback(save_interval_steps=250, save_total_limit=2),  # Adjusted callback\n    ],\n    compute_metrics=compute_metrics,\n)\n\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:32:05.019658Z","iopub.execute_input":"2024-11-22T22:32:05.020298Z","iopub.status.idle":"2024-11-22T22:32:05.065222Z","shell.execute_reply.started":"2024-11-22T22:32:05.020261Z","shell.execute_reply":"2024-11-22T22:32:05.064354Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"print(\"Fine-tuning SQuAD model...\")\ntrainer.train()\n\n# Save the final model\ntrainer.save_model(\"squad_final_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:32:10.334921Z","iopub.execute_input":"2024-11-22T22:32:10.335772Z","iopub.status.idle":"2024-11-23T02:52:17.476771Z","shell.execute_reply.started":"2024-11-22T22:32:10.335733Z","shell.execute_reply":"2024-11-23T02:52:17.475677Z"}},"outputs":[{"name":"stdout","text":"Fine-tuning SQuAD model...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1030' max='1030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1030/1030 4:19:35, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Logging metrics at step 200:\nModel saved at step 250: kaggle_model_results/model_step_250\nLogging metrics at step 400:\nLoss: 2.0202\nLearning rate: 4.029126213592233e-05\nMetrics: {'loss': 2.0202, 'grad_norm': 57560.171875, 'learning_rate': 4.029126213592233e-05, 'epoch': 0.1941747572815534, 'step': 200}\nModel saved at step 500: kaggle_model_results/model_step_500\nLogging metrics at step 600:\nLoss: 0.8009\nLearning rate: 3.058252427184466e-05\nMetrics: {'loss': 0.8009, 'grad_norm': 58916.6875, 'learning_rate': 3.058252427184466e-05, 'epoch': 0.3883495145631068, 'step': 400}\nModel saved at step 750: kaggle_model_results/model_step_750\nDeleted oldest checkpoint: kaggle_model_results/model_step_250\nLogging metrics at step 800:\nLoss: 0.7693\nLearning rate: 2.0873786407766992e-05\nMetrics: {'loss': 0.7693, 'grad_norm': 150360.828125, 'learning_rate': 2.0873786407766992e-05, 'epoch': 0.5825242718446602, 'step': 600}\nLogging metrics at step 1000:\nLoss: 0.7833\nLearning rate: 1.116504854368932e-05\nMetrics: {'loss': 0.7833, 'grad_norm': 51813.76171875, 'learning_rate': 1.116504854368932e-05, 'epoch': 0.7766990291262136, 'step': 800}\nModel saved at step 1000: kaggle_model_results/model_step_1000\nDeleted oldest checkpoint: kaggle_model_results/model_step_500\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## Upload to HF","metadata":{}},{"cell_type":"code","source":"repo_id = \"SumeetSawale/nlp_a3_squad_working\" \ncreate_repo(repo_id, repo_type=\"model\", exist_ok=True)  # Set exist_ok=True to avoid errors if it already exists\n\nprint(f\"Repository '{repo_id}' created successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:22:46.052433Z","iopub.execute_input":"2024-11-23T03:22:46.053232Z","iopub.status.idle":"2024-11-23T03:22:50.986850Z","shell.execute_reply.started":"2024-11-23T03:22:46.053195Z","shell.execute_reply":"2024-11-23T03:22:50.985912Z"}},"outputs":[{"name":"stdout","text":"Repository 'SumeetSawale/nlp_a3_squad_working' created successfully.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"folder_path = \"/kaggle/working\"\n\nupload_folder(\n    folder_path=folder_path,\n    repo_id=repo_id,\n    repo_type=\"model\",\n    commit_message=\"Upload full working directory\"\n)\n\nprint(f\"Model uploaded successfully to {repo_id}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:22:55.452587Z","iopub.execute_input":"2024-11-23T03:22:55.452980Z","iopub.status.idle":"2024-11-23T03:26:42.510271Z","shell.execute_reply.started":"2024-11-23T03:22:55.452946Z","shell.execute_reply":"2024-11-23T03:26:42.509309Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"189f043487d2480f8a0a815cfc4119db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd3f02cd403d41c993f1fe73428766f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 11 LFS files:   0%|          | 0/11 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15f3d0f1e1e14759a9073604ee2e80f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ace1a0906b2c408f9aa03e0fc12e4c6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"optimizer.pt:   0%|          | 0.00/35.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cab20f22c70448a8a531153dda7d6fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e6a1eaf435f4b7bad21549e9af19029"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e1bc6a5d42140678d9323283b1633ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1846c6f9ccd24668bebae9c8539b8541"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb379ddb549a4de5b35cd5eb65273dac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"147d7d7b2d024cac9f3d53fb1210fda1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b02dd2def3a46cba69375866ecefc4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3db4cb3f388f46a285aa66d96b9d3fa4"}},"metadata":{}},{"name":"stdout","text":"Model uploaded successfully to SumeetSawale/nlp_a3_squad_working.\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# Load the trained model and tokenizer\nmodel_checkpoint = \"squad_final_model\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n\n# Load SQuAD v2 dataset for validation\ndataset = load_dataset(\"squad_v2\", split=\"validation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:26:58.933746Z","iopub.execute_input":"2024-11-23T03:26:58.934072Z","iopub.status.idle":"2024-11-23T03:27:03.790888Z","shell.execute_reply.started":"2024-11-23T03:26:58.934044Z","shell.execute_reply":"2024-11-23T03:27:03.789963Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf4383dad989451e8ce5862403f84254"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/16.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2616091f182c4555a1d2f017cf2001f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9884f92b2d1e430ba8d4800beb6f4509"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0f89f17d0a54eb4939ff46afe6d2ec2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3276680f5944ca9947bfd609be1a484"}},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"# Use a pipeline for Question Answering\nqa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device = 0)\n\n# Helper function to generate predictions\ndef get_predictions(examples):\n    predictions = []\n    references = []\n\n    for i, example in enumerate(examples):\n        question = example['question']\n        context = example['context']\n\n        # Generate prediction using the QA pipeline\n        prediction = qa_pipeline({'question': question, 'context': context})\n        \n        # Store predictions and references\n        predictions.append(prediction['answer'])\n        references.append(example['answers']['text'][0] if example['answers']['text'] else \"\")\n\n        if(i%100 == 0):\n            print(f\"Predicted answer for {i} validation data\")\n    return predictions, references\n\n# Get predictions and references from the dataset\npredictions, references = get_predictions(datasets['validation'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T03:34:43.545541Z","iopub.execute_input":"2024-11-23T03:34:43.546194Z","iopub.status.idle":"2024-11-23T04:06:32.281337Z","shell.execute_reply.started":"2024-11-23T03:34:43.546157Z","shell.execute_reply":"2024-11-23T04:06:32.280633Z"}},"outputs":[{"name":"stdout","text":"Predicted answer for 0 validation data\nPredicted answer for 100 validation data\nPredicted answer for 200 validation data\nPredicted answer for 300 validation data\nPredicted answer for 400 validation data\nPredicted answer for 500 validation data\nPredicted answer for 600 validation data\nPredicted answer for 700 validation data\nPredicted answer for 800 validation data\nPredicted answer for 900 validation data\nPredicted answer for 1000 validation data\nPredicted answer for 1100 validation data\nPredicted answer for 1200 validation data\nPredicted answer for 1300 validation data\nPredicted answer for 1400 validation data\nPredicted answer for 1500 validation data\nPredicted answer for 1600 validation data\nPredicted answer for 1700 validation data\nPredicted answer for 1800 validation data\nPredicted answer for 1900 validation data\nPredicted answer for 2000 validation data\nPredicted answer for 2100 validation data\nPredicted answer for 2200 validation data\nPredicted answer for 2300 validation data\nPredicted answer for 2400 validation data\nPredicted answer for 2500 validation data\nPredicted answer for 2600 validation data\nPredicted answer for 2700 validation data\nPredicted answer for 2800 validation data\nPredicted answer for 2900 validation data\nPredicted answer for 3000 validation data\nPredicted answer for 3100 validation data\nPredicted answer for 3200 validation data\nPredicted answer for 3300 validation data\nPredicted answer for 3400 validation data\nPredicted answer for 3500 validation data\nPredicted answer for 3600 validation data\nPredicted answer for 3700 validation data\nPredicted answer for 3800 validation data\nPredicted answer for 3900 validation data\nPredicted answer for 4000 validation data\nPredicted answer for 4100 validation data\nPredicted answer for 4200 validation data\nPredicted answer for 4300 validation data\nPredicted answer for 4400 validation data\nPredicted answer for 4500 validation data\nPredicted answer for 4600 validation data\nPredicted answer for 4700 validation data\nPredicted answer for 4800 validation data\nPredicted answer for 4900 validation data\nPredicted answer for 5000 validation data\nPredicted answer for 5100 validation data\nPredicted answer for 5200 validation data\nPredicted answer for 5300 validation data\nPredicted answer for 5400 validation data\nPredicted answer for 5500 validation data\nPredicted answer for 5600 validation data\nPredicted answer for 5700 validation data\nPredicted answer for 5800 validation data\nPredicted answer for 5900 validation data\nPredicted answer for 6000 validation data\nPredicted answer for 6100 validation data\nPredicted answer for 6200 validation data\nPredicted answer for 6300 validation data\nPredicted answer for 6400 validation data\nPredicted answer for 6500 validation data\nPredicted answer for 6600 validation data\nPredicted answer for 6700 validation data\nPredicted answer for 6800 validation data\nPredicted answer for 6900 validation data\nPredicted answer for 7000 validation data\nPredicted answer for 7100 validation data\nPredicted answer for 7200 validation data\nPredicted answer for 7300 validation data\nPredicted answer for 7400 validation data\nPredicted answer for 7500 validation data\nPredicted answer for 7600 validation data\nPredicted answer for 7700 validation data\nPredicted answer for 7800 validation data\nPredicted answer for 7900 validation data\nPredicted answer for 8000 validation data\nPredicted answer for 8100 validation data\nPredicted answer for 8200 validation data\nPredicted answer for 8300 validation data\nPredicted answer for 8400 validation data\nPredicted answer for 8500 validation data\nPredicted answer for 8600 validation data\nPredicted answer for 8700 validation data\nPredicted answer for 8800 validation data\nPredicted answer for 8900 validation data\nPredicted answer for 9000 validation data\nPredicted answer for 9100 validation data\nPredicted answer for 9200 validation data\nPredicted answer for 9300 validation data\nPredicted answer for 9400 validation data\nPredicted answer for 9500 validation data\nPredicted answer for 9600 validation data\nPredicted answer for 9700 validation data\nPredicted answer for 9800 validation data\nPredicted answer for 9900 validation data\nPredicted answer for 10000 validation data\nPredicted answer for 10100 validation data\nPredicted answer for 10200 validation data\nPredicted answer for 10300 validation data\nPredicted answer for 10400 validation data\nPredicted answer for 10500 validation data\nPredicted answer for 10600 validation data\nPredicted answer for 10700 validation data\nPredicted answer for 10800 validation data\nPredicted answer for 10900 validation data\nPredicted answer for 11000 validation data\nPredicted answer for 11100 validation data\nPredicted answer for 11200 validation data\nPredicted answer for 11300 validation data\nPredicted answer for 11400 validation data\nPredicted answer for 11500 validation data\nPredicted answer for 11600 validation data\nPredicted answer for 11700 validation data\nPredicted answer for 11800 validation data\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"print(predictions[:10])\nprint(references[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T05:10:30.374453Z","iopub.execute_input":"2024-11-23T05:10:30.375223Z","iopub.status.idle":"2024-11-23T05:10:30.379762Z","shell.execute_reply.started":"2024-11-23T05:10:30.375184Z","shell.execute_reply":"2024-11-23T05:10:30.378817Z"}},"outputs":[{"name":"stdout","text":"['', '10', ' Normans', 'The Normans', ' and 11th centuries gave', '', ' Nourmands', ' to Normandy, a', ' Normands; Latin: Normanni) were', 'The']\n['France', '10th and 11th centuries', 'Denmark, Iceland and Norway', 'Rollo', '10th century', '', '', '', '', 'William the Conqueror']\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"from nltk.translate.meteor_score import meteor_score\nimport nltk\n# nltk.download('omw-1.4')  \n# nltk.download('punkt')   \nnltk.download('wordnet') \n\nimport nltk\nprint(nltk.data.path)\n\nimport nltk\nnltk.data.path.append('/usr/local/share/nltk_data')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:43:22.108528Z","iopub.execute_input":"2024-11-23T04:43:22.108941Z","iopub.status.idle":"2024-11-23T04:43:22.114288Z","shell.execute_reply.started":"2024-11-23T04:43:22.108907Z","shell.execute_reply":"2024-11-23T04:43:22.113341Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n['/root/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/root/nltk_data', '/root/nltk_data', '/root/nltk_data']\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"# Use a pipeline for Question Answering\nqa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device = 0)\n\n# Helper function to generate predictions\ndef get_predictions(examples):\n    predictions = []\n    references = []\n\n    for i, example in enumerate(examples):\n        question = example['question']\n        context = example['context']\n\n        # Generate prediction using the QA pipeline\n        prediction = qa_pipeline({'question': question, 'context': context})\n        \n        # Store predictions and references\n        predictions.append(prediction['answer'])\n        references.append(example['answers']['text'][0] if example['answers']['text'] else \"\")\n\n        if(i%100 == 0):\n            print(f\"Predicted answer for {i} validation data\")\n    return predictions, references\n\n# Get predictions and references from the dataset\npredictions, references = get_predictions(datasets['validation'].select(range(1000)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T05:42:59.093419Z","iopub.execute_input":"2024-11-23T05:42:59.094267Z","iopub.status.idle":"2024-11-23T05:44:48.213821Z","shell.execute_reply.started":"2024-11-23T05:42:59.094230Z","shell.execute_reply":"2024-11-23T05:44:48.212827Z"}},"outputs":[{"name":"stdout","text":"Predicted answer for 0 validation data\nPredicted answer for 100 validation data\nPredicted answer for 200 validation data\nPredicted answer for 300 validation data\nPredicted answer for 400 validation data\nPredicted answer for 500 validation data\nPredicted answer for 600 validation data\nPredicted answer for 700 validation data\nPredicted answer for 800 validation data\nPredicted answer for 900 validation data\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load evaluation metrics\nsquad_metric = load(\"squad\")\nbleu_metric = load(\"bleu\")\nmeteor_metric = load(\"meteor\")\nrouge_metric = load(\"rouge\")\n\n# Prepare the data for SQuAD metrics\nformatted_predictions = [{\"id\": str(i), \"prediction_text\": pred} for i, pred in enumerate(predictions)]\nreferences_squad = [{\"id\": str(i), \"answers\": {\"text\": [ref], \"answer_start\": [0]}} for i, ref in enumerate(references)]\n\n# Compute SQuAD metrics (Exact Match & F1)\nformatted_predictions = [\n    {\n        \"id\": str(i), \n        \"prediction_text\": pred, \n        \"no_answer_probability\": 0.0  # Default probability of no answer\n    } \n    for i, pred in enumerate(predictions)\n]\n\n# Compute BLEU, METEOR, and ROUGE metrics\nbleu_results = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in references])\nprint(\"\\nBLEU Score:\")\nprint(f\"BLEU: {bleu_results['bleu']}\")\nprint()\n\n\nrouge_results = rouge_metric.compute(predictions=predictions, references=references)\nprint(\"\\nROUGE Scores:\")\nprint(rouge_results)\nprint()\n\n# Print the results\nprint(\"SQuAD Metrics:\")\nprint(squad_results)\nprint()\n\nprint(f\"Exact Match (EM): {squad_results['exact']}\")\nprint()\nprint(f\"F1 Score:\\n {squad_results['f1']}\")\nprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T05:46:23.713012Z","iopub.execute_input":"2024-11-23T05:46:23.713776Z","iopub.status.idle":"2024-11-23T05:46:25.982236Z","shell.execute_reply.started":"2024-11-23T05:46:23.713739Z","shell.execute_reply":"2024-11-23T05:46:25.981434Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n\nBLEU Score:\nBLEU: 0.004169879925268008\n\n\nROUGE Scores:\n{'rouge1': 0.0288951878181569, 'rouge2': 0.009308885558885559, 'rougeL': 0.028590738250435024, 'rougeLsum': 0.02846869456185816}\n\nSQuAD Metrics:\n{'exact': 0.0, 'f1': 75.0, 'total': 1, 'HasAns_exact': 0.0, 'HasAns_f1': 75.0, 'HasAns_total': 1, 'best_exact': 0.0, 'best_exact_thresh': 0.0, 'best_f1': 75.0, 'best_f1_thresh': 0.0}\n\nExact Match (EM): 0.0\n\nF1 Score:\n 75.0\n\n","output_type":"stream"}],"execution_count":106},{"cell_type":"markdown","source":"## Metrics for untrained model","metadata":{}},{"cell_type":"code","source":"# Use a pipeline for Question Answering\nqa_pipeline2 = pipeline(\"question-answering\", model=AutoModelForQuestionAnswering.from_pretrained(model_checkpoint), tokenizer=AutoTokenizer.from_pretrained(model_checkpoint), device = 0)\n\n# Helper function to generate predictions\ndef get_predictions(examples):\n    predictions = []\n    references = []\n\n    for i, example in enumerate(examples):\n        question = example['question']\n        context = example['context']\n\n        # Generate prediction using the QA pipeline\n        prediction = qa_pipeline2({'question': question, 'context': context})\n        \n        # Store predictions and references\n        predictions.append(prediction['answer'])\n        references.append(example['answers']['text'][0] if example['answers']['text'] else \"\")\n\n        if(i%100 == 0):\n            print(f\"Predicted answer for {i} validation data\")\n    return predictions, references\n\n# Get predictions and references from the dataset\npredictions2, references2 = get_predictions(datasets['validation'].select(range(1000)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T05:47:25.407232Z","iopub.execute_input":"2024-11-23T05:47:25.408222Z","iopub.status.idle":"2024-11-23T05:49:22.262966Z","shell.execute_reply.started":"2024-11-23T05:47:25.408168Z","shell.execute_reply":"2024-11-23T05:49:22.262233Z"}},"outputs":[{"name":"stdout","text":"Predicted answer for 0 validation data\nPredicted answer for 100 validation data\nPredicted answer for 200 validation data\nPredicted answer for 300 validation data\nPredicted answer for 400 validation data\nPredicted answer for 500 validation data\nPredicted answer for 600 validation data\nPredicted answer for 700 validation data\nPredicted answer for 800 validation data\nPredicted answer for 900 validation data\n","output_type":"stream"}],"execution_count":107},{"cell_type":"code","source":"# Load evaluation metrics\nsquad_metric = load(\"squad\")\nbleu_metric = load(\"bleu\")\nmeteor_metric = load(\"meteor\")\nrouge_metric = load(\"rouge\")\n\n# Prepare the data for SQuAD metrics\nformatted_predictions = [{\"id\": str(i), \"prediction_text\": pred} for i, pred in enumerate(predictions)]\nreferences_squad = [{\"id\": str(i), \"answers\": {\"text\": [ref], \"answer_start\": [0]}} for i, ref in enumerate(references)]\n\n# Compute SQuAD metrics (Exact Match & F1)\nformatted_predictions = [\n    {\n        \"id\": str(i), \n        \"prediction_text\": pred, \n        \"no_answer_probability\": 0.0  # Default probability of no answer\n    } \n    for i, pred in enumerate(predictions)\n]\n\n# Compute BLEU, METEOR, and ROUGE metrics\nbleu_results = bleu_metric.compute(predictions=predictions, references=[ref for ref in references])\nprint(\"\\nBLEU Score:\")\nprint(f\"BLEU: {bleu_results['bleu']}\")\nprint()\n\n\nrouge_results = rouge_metric.compute(predictions=predictions, references=references)\nprint(\"\\nROUGE Scores:\")\nprint(rouge_results)\nprint()\n\n# Print the results\nprint(\"SQuAD Metrics:\")\nprint(squad_results)\nprint()\n\nprint(f\"Exact Match (EM): {squad_results['exact']}\")\nprint()\nprint(f\"F1 Score:\\n {squad_results['f1']}\")\nprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T05:54:52.210974Z","iopub.execute_input":"2024-11-23T05:54:52.211300Z","iopub.status.idle":"2024-11-23T05:54:55.351263Z","shell.execute_reply.started":"2024-11-23T05:54:52.211273Z","shell.execute_reply":"2024-11-23T05:54:55.350295Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n\nBLEU Score:\nBLEU: 0.004169879925268008\n\n\nROUGE Scores:\n{'rouge1': 0.0288951878181569, 'rouge2': 0.009308885558885559, 'rougeL': 0.028590738250435024, 'rougeLsum': 0.02846869456185816}\n\nSQuAD Metrics:\n{'exact': 0.0, 'f1': 75.0, 'total': 1, 'HasAns_exact': 0.0, 'HasAns_f1': 75.0, 'HasAns_total': 1, 'best_exact': 0.0, 'best_exact_thresh': 0.0, 'best_f1': 75.0, 'best_f1_thresh': 0.0}\n\nExact Match (EM): 0.0\n\nF1 Score:\n 75.0\n\n","output_type":"stream"}],"execution_count":109},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}