{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:43:06.484996Z","iopub.execute_input":"2024-11-22T06:43:06.485616Z","iopub.status.idle":"2024-11-22T06:44:12.669228Z","shell.execute_reply.started":"2024-11-22T06:43:06.485583Z","shell.execute_reply":"2024-11-22T06:44:12.668320Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nCollecting transformers\n  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed transformers-4.46.3\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nCollecting datasets\n  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.0.1\n    Uninstalling datasets-3.0.1:\n      Successfully uninstalled datasets-3.0.1\nSuccessfully installed datasets-3.1.0\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nCollecting accelerate\n  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.25.1)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.5)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.34.2\n    Uninstalling accelerate-0.34.2:\n      Successfully uninstalled accelerate-0.34.2\nSuccessfully installed accelerate-1.1.1\nNote: you may need to restart the kernel to use updated packages.\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.1.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\nNote: you may need to restart the kernel to use updated packages.\nCollecting trl\n  Downloading trl-0.12.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: accelerate>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from trl) (1.1.1)\nRequirement already satisfied: datasets>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from trl) (3.1.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl) (13.7.1)\nRequirement already satisfied: transformers>=4.46.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.46.3)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (0.25.1)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (0.4.5)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.9.5)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.46.0->trl) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.46.0->trl) (0.20.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (2.18.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.34.0->trl) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\nDownloading trl-0.12.1-py3-none-any.whl (310 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.9/310.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: trl\nSuccessfully installed trl-0.12.1\nNote: you may need to restart the kernel to use updated packages.\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Necessary imports","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging\n    \n    )\n    \nfrom peft import (LoraConfig,\n                 PeftModel,\n                 prepare_model_for_kbit_training,\n                 get_peft_model)\n\nimport os \nimport torch\nimport wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:44:12.670891Z","iopub.execute_input":"2024-11-22T06:44:12.671202Z","iopub.status.idle":"2024-11-22T06:44:30.988865Z","shell.execute_reply.started":"2024-11-22T06:44:12.671172Z","shell.execute_reply":"2024-11-22T06:44:30.987948Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Huggingface login","metadata":{}},{"cell_type":"code","source":"# insert token\n\n!huggingface-cli login --token=","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:44:30.989927Z","iopub.execute_input":"2024-11-22T06:44:30.990489Z","iopub.status.idle":"2024-11-22T06:44:32.588876Z","shell.execute_reply.started":"2024-11-22T06:44:30.990460Z","shell.execute_reply":"2024-11-22T06:44:32.587793Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Install evaluate library","metadata":{}},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:44:32.591033Z","iopub.execute_input":"2024-11-22T06:44:32.591333Z","iopub.status.idle":"2024-11-22T06:44:42.537242Z","shell.execute_reply.started":"2024-11-22T06:44:32.591303Z","shell.execute_reply":"2024-11-22T06:44:42.536369Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## SST2 dataset","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    TrainingArguments, \n    Trainer\n)\nfrom datasets import load_dataset\nimport numpy as np\nimport evaluate\n\ndataset = load_dataset(\"glue\", \"sst2\")\n\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:44:42.538559Z","iopub.execute_input":"2024-11-22T06:44:42.538818Z","iopub.status.idle":"2024-11-22T06:44:46.322328Z","shell.execute_reply.started":"2024-11-22T06:44:42.538791Z","shell.execute_reply":"2024-11-22T06:44:46.321525Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f012198cb0ba4efd9732ad9198bbea14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb018d62792043b0907e5a94fc22b6e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70aa6277663a4475a26df5695e0f969e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d6d1e92376d4d648fdd7ec829f31cf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"373133f5554f4a1bbf9d2e668bdc4137"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"904944dcc38744118cf2fee1e132240b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1f1f6b606464c9f82c61f7ac67cff54"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 67349\n    })\n    validation: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 872\n    })\n    test: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 1821\n    })\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\ncombined_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]])\n\ncombined_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:49:10.614838Z","iopub.execute_input":"2024-11-22T06:49:10.615541Z","iopub.status.idle":"2024-11-22T06:49:10.627400Z","shell.execute_reply.started":"2024-11-22T06:49:10.615502Z","shell.execute_reply":"2024-11-22T06:49:10.626625Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['sentence', 'label', 'idx'],\n    num_rows: 70042\n})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"dataset = combined_dataset.train_test_split(test_size=0.2, seed=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:49:12.958098Z","iopub.execute_input":"2024-11-22T06:49:12.958768Z","iopub.status.idle":"2024-11-22T06:49:12.986612Z","shell.execute_reply.started":"2024-11-22T06:49:12.958737Z","shell.execute_reply":"2024-11-22T06:49:12.985777Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:49:14.204102Z","iopub.execute_input":"2024-11-22T06:49:14.204457Z","iopub.status.idle":"2024-11-22T06:49:14.210026Z","shell.execute_reply.started":"2024-11-22T06:49:14.204428Z","shell.execute_reply":"2024-11-22T06:49:14.209223Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 56033\n    })\n    test: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 14009\n    })\n})"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"# Analyzing the base model","metadata":{}},{"cell_type":"code","source":"base_model_llama = \"meta-llama/Llama-3.2-1B\"\ntokenizer_llama = AutoTokenizer.from_pretrained(base_model_llama, trust_remote_code=True)\ntokenizer_llama.pad_token = tokenizer_llama.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:50:28.978510Z","iopub.execute_input":"2024-11-22T06:50:28.978876Z","iopub.status.idle":"2024-11-22T06:50:29.450842Z","shell.execute_reply.started":"2024-11-22T06:50:28.978843Z","shell.execute_reply":"2024-11-22T06:50:29.449758Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## Calculating number of parameters","metadata":{}},{"cell_type":"code","source":"def count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        # if not parameter.requires_grad:\n        #     continue\n        params = parameter.numel()\n        table.add_row([name, params])\n        total_params += params\n    print(table)\n    print(f\"Total Params: {total_params}\")\n    return total_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:50:42.043052Z","iopub.execute_input":"2024-11-22T06:50:42.043401Z","iopub.status.idle":"2024-11-22T06:50:42.048561Z","shell.execute_reply.started":"2024-11-22T06:50:42.043372Z","shell.execute_reply":"2024-11-22T06:50:42.047576Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### Base model parameters","metadata":{}},{"cell_type":"code","source":"from prettytable import PrettyTable\ncount_parameters(AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:51:06.304468Z","iopub.execute_input":"2024-11-22T06:51:06.304829Z","iopub.status.idle":"2024-11-22T06:51:09.850801Z","shell.execute_reply.started":"2024-11-22T06:51:06.304795Z","shell.execute_reply":"2024-11-22T06:51:09.849922Z"}},"outputs":[{"name":"stdout","text":"+-------------------------------------------------+------------+\n|                     Modules                     | Parameters |\n+-------------------------------------------------+------------+\n|            model.embed_tokens.weight            | 262668288  |\n|      model.layers.0.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.0.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.0.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.0.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.0.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.0.mlp.up_proj.weight        |  16777216  |\n|       model.layers.0.mlp.down_proj.weight       |  16777216  |\n|      model.layers.0.input_layernorm.weight      |    2048    |\n|  model.layers.0.post_attention_layernorm.weight |    2048    |\n|      model.layers.1.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.1.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.1.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.1.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.1.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.1.mlp.up_proj.weight        |  16777216  |\n|       model.layers.1.mlp.down_proj.weight       |  16777216  |\n|      model.layers.1.input_layernorm.weight      |    2048    |\n|  model.layers.1.post_attention_layernorm.weight |    2048    |\n|      model.layers.2.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.2.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.2.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.2.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.2.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.2.mlp.up_proj.weight        |  16777216  |\n|       model.layers.2.mlp.down_proj.weight       |  16777216  |\n|      model.layers.2.input_layernorm.weight      |    2048    |\n|  model.layers.2.post_attention_layernorm.weight |    2048    |\n|      model.layers.3.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.3.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.3.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.3.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.3.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.3.mlp.up_proj.weight        |  16777216  |\n|       model.layers.3.mlp.down_proj.weight       |  16777216  |\n|      model.layers.3.input_layernorm.weight      |    2048    |\n|  model.layers.3.post_attention_layernorm.weight |    2048    |\n|      model.layers.4.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.4.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.4.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.4.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.4.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.4.mlp.up_proj.weight        |  16777216  |\n|       model.layers.4.mlp.down_proj.weight       |  16777216  |\n|      model.layers.4.input_layernorm.weight      |    2048    |\n|  model.layers.4.post_attention_layernorm.weight |    2048    |\n|      model.layers.5.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.5.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.5.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.5.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.5.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.5.mlp.up_proj.weight        |  16777216  |\n|       model.layers.5.mlp.down_proj.weight       |  16777216  |\n|      model.layers.5.input_layernorm.weight      |    2048    |\n|  model.layers.5.post_attention_layernorm.weight |    2048    |\n|      model.layers.6.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.6.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.6.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.6.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.6.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.6.mlp.up_proj.weight        |  16777216  |\n|       model.layers.6.mlp.down_proj.weight       |  16777216  |\n|      model.layers.6.input_layernorm.weight      |    2048    |\n|  model.layers.6.post_attention_layernorm.weight |    2048    |\n|      model.layers.7.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.7.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.7.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.7.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.7.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.7.mlp.up_proj.weight        |  16777216  |\n|       model.layers.7.mlp.down_proj.weight       |  16777216  |\n|      model.layers.7.input_layernorm.weight      |    2048    |\n|  model.layers.7.post_attention_layernorm.weight |    2048    |\n|      model.layers.8.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.8.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.8.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.8.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.8.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.8.mlp.up_proj.weight        |  16777216  |\n|       model.layers.8.mlp.down_proj.weight       |  16777216  |\n|      model.layers.8.input_layernorm.weight      |    2048    |\n|  model.layers.8.post_attention_layernorm.weight |    2048    |\n|      model.layers.9.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.9.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.9.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.9.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.9.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.9.mlp.up_proj.weight        |  16777216  |\n|       model.layers.9.mlp.down_proj.weight       |  16777216  |\n|      model.layers.9.input_layernorm.weight      |    2048    |\n|  model.layers.9.post_attention_layernorm.weight |    2048    |\n|     model.layers.10.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.10.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.10.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.10.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.10.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.10.mlp.up_proj.weight       |  16777216  |\n|       model.layers.10.mlp.down_proj.weight      |  16777216  |\n|      model.layers.10.input_layernorm.weight     |    2048    |\n| model.layers.10.post_attention_layernorm.weight |    2048    |\n|     model.layers.11.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.11.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.11.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.11.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.11.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.11.mlp.up_proj.weight       |  16777216  |\n|       model.layers.11.mlp.down_proj.weight      |  16777216  |\n|      model.layers.11.input_layernorm.weight     |    2048    |\n| model.layers.11.post_attention_layernorm.weight |    2048    |\n|     model.layers.12.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.12.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.12.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.12.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.12.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.12.mlp.up_proj.weight       |  16777216  |\n|       model.layers.12.mlp.down_proj.weight      |  16777216  |\n|      model.layers.12.input_layernorm.weight     |    2048    |\n| model.layers.12.post_attention_layernorm.weight |    2048    |\n|     model.layers.13.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.13.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.13.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.13.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.13.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.13.mlp.up_proj.weight       |  16777216  |\n|       model.layers.13.mlp.down_proj.weight      |  16777216  |\n|      model.layers.13.input_layernorm.weight     |    2048    |\n| model.layers.13.post_attention_layernorm.weight |    2048    |\n|     model.layers.14.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.14.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.14.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.14.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.14.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.14.mlp.up_proj.weight       |  16777216  |\n|       model.layers.14.mlp.down_proj.weight      |  16777216  |\n|      model.layers.14.input_layernorm.weight     |    2048    |\n| model.layers.14.post_attention_layernorm.weight |    2048    |\n|     model.layers.15.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.15.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.15.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.15.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.15.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.15.mlp.up_proj.weight       |  16777216  |\n|       model.layers.15.mlp.down_proj.weight      |  16777216  |\n|      model.layers.15.input_layernorm.weight     |    2048    |\n| model.layers.15.post_attention_layernorm.weight |    2048    |\n|                model.norm.weight                |    2048    |\n+-------------------------------------------------+------------+\nTotal Params: 1235814400\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"1235814400"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"### AutoModelForSequenceClassification model parameters","metadata":{}},{"cell_type":"code","source":"model_llama = AutoModelForSequenceClassification.from_pretrained(base_model_llama, num_labels=2)\n\ncount_parameters(model_llama)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T06:52:04.680469Z","iopub.execute_input":"2024-11-22T06:52:04.680825Z","iopub.status.idle":"2024-11-22T06:52:06.819137Z","shell.execute_reply.started":"2024-11-22T06:52:04.680794Z","shell.execute_reply":"2024-11-22T06:52:06.818196Z"}},"outputs":[{"name":"stderr","text":"Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"+-------------------------------------------------+------------+\n|                     Modules                     | Parameters |\n+-------------------------------------------------+------------+\n|            model.embed_tokens.weight            | 262668288  |\n|      model.layers.0.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.0.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.0.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.0.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.0.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.0.mlp.up_proj.weight        |  16777216  |\n|       model.layers.0.mlp.down_proj.weight       |  16777216  |\n|      model.layers.0.input_layernorm.weight      |    2048    |\n|  model.layers.0.post_attention_layernorm.weight |    2048    |\n|      model.layers.1.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.1.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.1.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.1.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.1.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.1.mlp.up_proj.weight        |  16777216  |\n|       model.layers.1.mlp.down_proj.weight       |  16777216  |\n|      model.layers.1.input_layernorm.weight      |    2048    |\n|  model.layers.1.post_attention_layernorm.weight |    2048    |\n|      model.layers.2.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.2.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.2.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.2.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.2.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.2.mlp.up_proj.weight        |  16777216  |\n|       model.layers.2.mlp.down_proj.weight       |  16777216  |\n|      model.layers.2.input_layernorm.weight      |    2048    |\n|  model.layers.2.post_attention_layernorm.weight |    2048    |\n|      model.layers.3.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.3.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.3.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.3.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.3.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.3.mlp.up_proj.weight        |  16777216  |\n|       model.layers.3.mlp.down_proj.weight       |  16777216  |\n|      model.layers.3.input_layernorm.weight      |    2048    |\n|  model.layers.3.post_attention_layernorm.weight |    2048    |\n|      model.layers.4.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.4.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.4.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.4.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.4.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.4.mlp.up_proj.weight        |  16777216  |\n|       model.layers.4.mlp.down_proj.weight       |  16777216  |\n|      model.layers.4.input_layernorm.weight      |    2048    |\n|  model.layers.4.post_attention_layernorm.weight |    2048    |\n|      model.layers.5.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.5.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.5.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.5.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.5.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.5.mlp.up_proj.weight        |  16777216  |\n|       model.layers.5.mlp.down_proj.weight       |  16777216  |\n|      model.layers.5.input_layernorm.weight      |    2048    |\n|  model.layers.5.post_attention_layernorm.weight |    2048    |\n|      model.layers.6.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.6.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.6.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.6.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.6.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.6.mlp.up_proj.weight        |  16777216  |\n|       model.layers.6.mlp.down_proj.weight       |  16777216  |\n|      model.layers.6.input_layernorm.weight      |    2048    |\n|  model.layers.6.post_attention_layernorm.weight |    2048    |\n|      model.layers.7.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.7.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.7.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.7.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.7.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.7.mlp.up_proj.weight        |  16777216  |\n|       model.layers.7.mlp.down_proj.weight       |  16777216  |\n|      model.layers.7.input_layernorm.weight      |    2048    |\n|  model.layers.7.post_attention_layernorm.weight |    2048    |\n|      model.layers.8.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.8.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.8.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.8.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.8.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.8.mlp.up_proj.weight        |  16777216  |\n|       model.layers.8.mlp.down_proj.weight       |  16777216  |\n|      model.layers.8.input_layernorm.weight      |    2048    |\n|  model.layers.8.post_attention_layernorm.weight |    2048    |\n|      model.layers.9.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.9.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.9.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.9.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.9.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.9.mlp.up_proj.weight        |  16777216  |\n|       model.layers.9.mlp.down_proj.weight       |  16777216  |\n|      model.layers.9.input_layernorm.weight      |    2048    |\n|  model.layers.9.post_attention_layernorm.weight |    2048    |\n|     model.layers.10.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.10.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.10.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.10.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.10.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.10.mlp.up_proj.weight       |  16777216  |\n|       model.layers.10.mlp.down_proj.weight      |  16777216  |\n|      model.layers.10.input_layernorm.weight     |    2048    |\n| model.layers.10.post_attention_layernorm.weight |    2048    |\n|     model.layers.11.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.11.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.11.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.11.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.11.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.11.mlp.up_proj.weight       |  16777216  |\n|       model.layers.11.mlp.down_proj.weight      |  16777216  |\n|      model.layers.11.input_layernorm.weight     |    2048    |\n| model.layers.11.post_attention_layernorm.weight |    2048    |\n|     model.layers.12.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.12.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.12.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.12.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.12.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.12.mlp.up_proj.weight       |  16777216  |\n|       model.layers.12.mlp.down_proj.weight      |  16777216  |\n|      model.layers.12.input_layernorm.weight     |    2048    |\n| model.layers.12.post_attention_layernorm.weight |    2048    |\n|     model.layers.13.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.13.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.13.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.13.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.13.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.13.mlp.up_proj.weight       |  16777216  |\n|       model.layers.13.mlp.down_proj.weight      |  16777216  |\n|      model.layers.13.input_layernorm.weight     |    2048    |\n| model.layers.13.post_attention_layernorm.weight |    2048    |\n|     model.layers.14.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.14.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.14.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.14.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.14.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.14.mlp.up_proj.weight       |  16777216  |\n|       model.layers.14.mlp.down_proj.weight      |  16777216  |\n|      model.layers.14.input_layernorm.weight     |    2048    |\n| model.layers.14.post_attention_layernorm.weight |    2048    |\n|     model.layers.15.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.15.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.15.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.15.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.15.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.15.mlp.up_proj.weight       |  16777216  |\n|       model.layers.15.mlp.down_proj.weight      |  16777216  |\n|      model.layers.15.input_layernorm.weight     |    2048    |\n| model.layers.15.post_attention_layernorm.weight |    2048    |\n|                model.norm.weight                |    2048    |\n|                   score.weight                  |    4096    |\n+-------------------------------------------------+------------+\nTotal Params: 1235818496\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"1235818496"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"## Fine Tuning the score layer","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    TrainingArguments, \n    Trainer\n)\nfrom datasets import load_dataset\nimport numpy as np\nimport evaluate\n\ndataset = load_dataset(\"glue\", \"sst2\")\n\nbase_model = \"meta-llama/Llama-3.2-1B\"  # Replace with the correct model name\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token  # Use EOS token as pad token if missing\n\ndef preprocess_data(examples):\n    tokens = tokenizer(\n        examples[\"sentence\"],\n        padding=\"max_length\",\n        truncation=False,  # Truncate sequences exceeding max_length\n        return_overflowing_tokens=True,\n        max_length=128,  # Adjust based on the model's capacity\n        stride=64\n    )\n    tokens[\"labels\"] = examples[\"label\"]\n    return tokens\n\ntokenized_datasets = dataset.map(preprocess_data, batched=True)\n\ntokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\ntokenized_datasets.set_format(\"torch\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(base_model, num_labels=2)\n\nfor name, param in model.named_parameters():\n    if \"score\" not in name:  # The classifier layer has \"score\" in its name\n        param.requires_grad = False\n\nmetric = evaluate.load(\"accuracy\")\n\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = accuracy_score(labels, predictions)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n    return {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }\n\nfrom transformers import TrainerCallback\n\nclass CustomLoggingCallback(TrainerCallback):\n    def __init__(self, log_interval=300):\n        super().__init__()\n        self.log_interval = log_interval\n\n    def on_step_end(self, args, state, control, **kwargs):\n        if state.global_step % self.log_interval == 0:\n            print(f\"Logging metrics at step {state.global_step}:\")\n            print(f\"Loss: {state.log_history[-1]['loss']}\")\n            print(f\"Learning rate: {state.log_history[-1].get('learning_rate', 'N/A')}\")\n            print(f\"Metrics: {state.log_history[-1]}\")\n\nfrom transformers import Trainer, DataCollatorForLanguageModeling, TrainingArguments, TrainerCallback\n\nclass ModelSaveCallback(TrainerCallback):\n    def __init__(self, save_interval=0.25):\n        super().__init__()\n        self.save_interval = save_interval  # Interval to save model (fraction of epoch)\n        self.last_saved_step = 0\n\n    def on_step_end(self, args, state, control, **kwargs):\n        current_epoch_fraction = state.global_step / state.max_steps * args.num_train_epochs\n\n        if (current_epoch_fraction - self.last_saved_step) >= self.save_interval:\n            self.last_saved_step = current_epoch_fraction  # Update the last saved step\n\n            model_save_path = f\"{wandb.run.dir}/model_epoch_{current_epoch_fraction:.2f}\"\n            model.save_pretrained(model_save_path)\n\n            wandb.save(f\"{model_save_path}/*\")  # Save all files in the directory to W&B\n            print(f\"Model saved at {current_epoch_fraction:.2f} epochs\")\n\ntraining_args = TrainingArguments(\n    output_dir=\"./sst2_llama_model2\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,  # Adjust based on available resources\n    per_device_eval_batch_size=16,\n    # gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    fp16=True,  # Enable mixed precision training\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    report_to=None  # Disable W&B for simplicity; use \"wandb\" if enabled\n)\n\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    # callbacks=[ModelSaveCallback(save_interval=0.25), CustomLoggingCallback(log_interval=100)],\n)\n\ntrainer.train()\n\neval_results = trainer.evaluate()\nprint(f\"Evaluation Results: {eval_results}\")\n\nmodel.save_pretrained(\"./sst2_llama_model2\")\ntokenizer.save_pretrained(\"./sst2_llama_model2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T06:31:07.818627Z","iopub.execute_input":"2024-11-20T06:31:07.818991Z","iopub.status.idle":"2024-11-20T10:22:39.793969Z","shell.execute_reply.started":"2024-11-20T06:31:07.818958Z","shell.execute_reply":"2024-11-20T10:22:39.792977Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edf864b3e1e9445faf7039cd83966994"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc3212e18d6c42e8abce109dc639b766"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"992b6ee661a3437bba677b4c8bf6ec41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a1ed0fd53b34d38869794f8e1e7358a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2fce8d419944db9b9a75e9b6df9eb27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6033cd38ff1046a39823adad0703a1df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bebe7b628fa4c609186974cb183884d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c012495410045ba84c664395e606844"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b93e55370a74d6ea370732d808cd702"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07129671c8a649a8beedb2aef49908ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db75804527734b97ab1f86165b9a7d70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92cb38c5b50b485e8ecf1fe5f0d735a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2840a0123934e00b91c55a40e21e814"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a61306c4c06d4486b6e6fd82702bd2e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43ada42717a0452d94177bc36b561403"}},"metadata":{}},{"name":"stderr","text":"Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddf9ea0ce6b0454e8cca2fc253381b6a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/tmp/ipykernel_30/1548526749.py:126: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114478766666656, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fd99a43419d4d4aa8a7320ad1279bbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241120_063232-psm9kk1n</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sumeetsawale-indian-institute-of-technology-gandhinagar/huggingface/runs/psm9kk1n' target=\"_blank\">./sst2_llama_model2</a></strong> to <a href='https://wandb.ai/sumeetsawale-indian-institute-of-technology-gandhinagar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sumeetsawale-indian-institute-of-technology-gandhinagar/huggingface' target=\"_blank\">https://wandb.ai/sumeetsawale-indian-institute-of-technology-gandhinagar/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sumeetsawale-indian-institute-of-technology-gandhinagar/huggingface/runs/psm9kk1n' target=\"_blank\">https://wandb.ai/sumeetsawale-indian-institute-of-technology-gandhinagar/huggingface/runs/psm9kk1n</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6315' max='6315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6315/6315 3:48:59, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.427300</td>\n      <td>0.419555</td>\n      <td>0.811927</td>\n      <td>0.816742</td>\n      <td>0.813063</td>\n      <td>0.814898</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.367200</td>\n      <td>0.381401</td>\n      <td>0.838303</td>\n      <td>0.846682</td>\n      <td>0.833333</td>\n      <td>0.839955</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.376100</td>\n      <td>0.372939</td>\n      <td>0.845183</td>\n      <td>0.850340</td>\n      <td>0.844595</td>\n      <td>0.847458</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.372938871383667, 'eval_accuracy': 0.8451834862385321, 'eval_precision': 0.8503401360544217, 'eval_recall': 0.8445945945945946, 'eval_f1': 0.847457627118644, 'eval_runtime': 48.3783, 'eval_samples_per_second': 18.025, 'eval_steps_per_second': 0.579, 'epoch': 3.0}\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"('./sst2_llama_model2/tokenizer_config.json',\n './sst2_llama_model2/special_tokens_map.json',\n './sst2_llama_model2/tokenizer.json')"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"model.config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T14:01:16.723746Z","iopub.execute_input":"2024-11-20T14:01:16.724377Z","iopub.status.idle":"2024-11-20T14:01:16.730854Z","shell.execute_reply.started":"2024-11-20T14:01:16.724342Z","shell.execute_reply":"2024-11-20T14:01:16.730094Z"}},"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"LlamaConfig {\n  \"_name_or_path\": \"meta-llama/Llama-3.2-1B\",\n  \"architectures\": [\n    \"LlamaForSequenceClassification\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128001,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 16,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128001,\n  \"pretraining_tp\": 1,\n  \"problem_type\": \"single_label_classification\",\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}"},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T14:01:26.957097Z","iopub.execute_input":"2024-11-20T14:01:26.957702Z","iopub.status.idle":"2024-11-20T14:01:26.965853Z","shell.execute_reply.started":"2024-11-20T14:01:26.957667Z","shell.execute_reply":"2024-11-20T14:01:26.964991Z"}},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"LlamaForSequenceClassification(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (score): Linear(in_features=2048, out_features=2, bias=False)\n)"},"metadata":{}}],"execution_count":66},{"cell_type":"markdown","source":"## Upload the model to huggingface","metadata":{}},{"cell_type":"markdown","source":"### Create a new repo","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import create_repo\n\nrepo_id = \"SumeetSawale/nlp_a3_sst_working\"  # Update with your desired repository name\ncreate_repo(repo_id, repo_type=\"model\", exist_ok=True)  # Set exist_ok=True to avoid errors if it already exists\n\nprint(f\"Repository '{repo_id}' created successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T12:54:00.281266Z","iopub.execute_input":"2024-11-20T12:54:00.281664Z","iopub.status.idle":"2024-11-20T12:54:03.737580Z","shell.execute_reply.started":"2024-11-20T12:54:00.281629Z","shell.execute_reply":"2024-11-20T12:54:03.736755Z"}},"outputs":[{"name":"stdout","text":"Repository 'SumeetSawale/nlp_a3_sst_working' created successfully.\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"### Push files to the repo","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import upload_folder\n\nfolder_path = \"/kaggle/working\"\n\nupload_folder(\n    folder_path=folder_path,\n    repo_id=repo_id,\n    repo_type=\"model\",\n    commit_message=\"Upload full working directory\"\n)\n\nprint(f\"Model uploaded successfully to {repo_id}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T12:54:08.277367Z","iopub.execute_input":"2024-11-20T12:54:08.277741Z","iopub.status.idle":"2024-11-20T12:56:39.781452Z","shell.execute_reply.started":"2024-11-20T12:54:08.277708Z","shell.execute_reply":"2024-11-20T12:56:39.780545Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Upload 17 LFS files:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed858c2fd5d24dc58c9857de05a8e850"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1732084343.4e81e9dea07a.30.0:   0%|          | 0.00/33.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2c28726573840fba74171f5025c7068"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3da744aca3aa49c9a0b500f814720c3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1732098146.4e81e9dea07a.30.1:   0%|          | 0.00/560 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d143662b9fc42c08e557e0ac13bf25c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b96941c9b6840ca9b0baf12c63eaee7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"optimizer.pt:   0%|          | 0.00/34.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6457735ae5a4b68b0bb45f07727eb90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96018131912e43b98bf63eb0e597ffe1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd0ef1ee679348649b7172bb3574710a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"579888792d124866a36fc8d582723c32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1299af57499e408c82eba69bd6cb44a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"optimizer.pt:   0%|          | 0.00/34.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e5bd14115714affb9854fa467f1e2a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d4e42226d614775908c65be69433160"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c0090991140434fbcee782750c588be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3438a56279f54a7fa09f9cc5f38b423c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76ffbbe8db6641da8676c35f197967a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44790e5a46604fcdb3f073f5839d3935"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa494b0ce23a4f1082a7049cf3bbefc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"run-psm9kk1n.wandb:   0%|          | 0.00/7.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fafbbdf83bd24e47831df3fc06517269"}},"metadata":{}},{"name":"stdout","text":"Model uploaded successfully to SumeetSawale/nlp_a3_sst_working.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"## Analyze number of parameters in fine-tuned model","metadata":{}},{"cell_type":"code","source":"!pip install prettytable","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T11:18:33.006709Z","iopub.execute_input":"2024-11-20T11:18:33.007415Z","iopub.status.idle":"2024-11-20T11:18:41.630109Z","shell.execute_reply.started":"2024-11-20T11:18:33.007364Z","shell.execute_reply":"2024-11-20T11:18:41.629021Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: prettytable in /opt/conda/lib/python3.10/site-packages (3.10.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prettytable) (0.2.13)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from prettytable import PrettyTable","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T11:18:54.243382Z","iopub.execute_input":"2024-11-20T11:18:54.244156Z","iopub.status.idle":"2024-11-20T11:18:54.272510Z","shell.execute_reply.started":"2024-11-20T11:18:54.244115Z","shell.execute_reply":"2024-11-20T11:18:54.271585Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### Check trainable params","metadata":{}},{"cell_type":"code","source":"def count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if not parameter.requires_grad:\n            continue\n        params = parameter.numel()\n        table.add_row([name, params])\n        total_params += params\n    print(table)\n    print(f\"Total Trainable Params: {total_params}\")\n    return total_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T11:18:58.260201Z","iopub.execute_input":"2024-11-20T11:18:58.260551Z","iopub.status.idle":"2024-11-20T11:18:58.266228Z","shell.execute_reply.started":"2024-11-20T11:18:58.260522Z","shell.execute_reply":"2024-11-20T11:18:58.265511Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"count_parameters(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T11:19:06.354083Z","iopub.execute_input":"2024-11-20T11:19:06.354447Z","iopub.status.idle":"2024-11-20T11:19:06.366834Z","shell.execute_reply.started":"2024-11-20T11:19:06.354417Z","shell.execute_reply":"2024-11-20T11:19:06.366017Z"}},"outputs":[{"name":"stdout","text":"+--------------+------------+\n|   Modules    | Parameters |\n+--------------+------------+\n| score.weight |    4096    |\n+--------------+------------+\nTotal Trainable Params: 4096\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"4096"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"### Check all params","metadata":{}},{"cell_type":"code","source":"def count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        # if not parameter.requires_grad:\n        #     continue\n        params = parameter.numel()\n        table.add_row([name, params])\n        total_params += params\n    print(table)\n    print(f\"Total Params: {total_params}\")\n    return total_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T11:51:13.992736Z","iopub.execute_input":"2024-11-20T11:51:13.993089Z","iopub.status.idle":"2024-11-20T11:51:13.998695Z","shell.execute_reply.started":"2024-11-20T11:51:13.993058Z","shell.execute_reply":"2024-11-20T11:51:13.998008Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"count_parameters(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T11:51:15.078368Z","iopub.execute_input":"2024-11-20T11:51:15.078688Z","iopub.status.idle":"2024-11-20T11:51:15.100006Z","shell.execute_reply.started":"2024-11-20T11:51:15.078661Z","shell.execute_reply":"2024-11-20T11:51:15.099234Z"}},"outputs":[{"name":"stdout","text":"+-------------------------------------------------+------------+\n|                     Modules                     | Parameters |\n+-------------------------------------------------+------------+\n|            model.embed_tokens.weight            | 262668288  |\n|      model.layers.0.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.0.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.0.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.0.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.0.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.0.mlp.up_proj.weight        |  16777216  |\n|       model.layers.0.mlp.down_proj.weight       |  16777216  |\n|      model.layers.0.input_layernorm.weight      |    2048    |\n|  model.layers.0.post_attention_layernorm.weight |    2048    |\n|      model.layers.1.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.1.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.1.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.1.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.1.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.1.mlp.up_proj.weight        |  16777216  |\n|       model.layers.1.mlp.down_proj.weight       |  16777216  |\n|      model.layers.1.input_layernorm.weight      |    2048    |\n|  model.layers.1.post_attention_layernorm.weight |    2048    |\n|      model.layers.2.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.2.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.2.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.2.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.2.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.2.mlp.up_proj.weight        |  16777216  |\n|       model.layers.2.mlp.down_proj.weight       |  16777216  |\n|      model.layers.2.input_layernorm.weight      |    2048    |\n|  model.layers.2.post_attention_layernorm.weight |    2048    |\n|      model.layers.3.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.3.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.3.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.3.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.3.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.3.mlp.up_proj.weight        |  16777216  |\n|       model.layers.3.mlp.down_proj.weight       |  16777216  |\n|      model.layers.3.input_layernorm.weight      |    2048    |\n|  model.layers.3.post_attention_layernorm.weight |    2048    |\n|      model.layers.4.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.4.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.4.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.4.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.4.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.4.mlp.up_proj.weight        |  16777216  |\n|       model.layers.4.mlp.down_proj.weight       |  16777216  |\n|      model.layers.4.input_layernorm.weight      |    2048    |\n|  model.layers.4.post_attention_layernorm.weight |    2048    |\n|      model.layers.5.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.5.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.5.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.5.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.5.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.5.mlp.up_proj.weight        |  16777216  |\n|       model.layers.5.mlp.down_proj.weight       |  16777216  |\n|      model.layers.5.input_layernorm.weight      |    2048    |\n|  model.layers.5.post_attention_layernorm.weight |    2048    |\n|      model.layers.6.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.6.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.6.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.6.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.6.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.6.mlp.up_proj.weight        |  16777216  |\n|       model.layers.6.mlp.down_proj.weight       |  16777216  |\n|      model.layers.6.input_layernorm.weight      |    2048    |\n|  model.layers.6.post_attention_layernorm.weight |    2048    |\n|      model.layers.7.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.7.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.7.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.7.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.7.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.7.mlp.up_proj.weight        |  16777216  |\n|       model.layers.7.mlp.down_proj.weight       |  16777216  |\n|      model.layers.7.input_layernorm.weight      |    2048    |\n|  model.layers.7.post_attention_layernorm.weight |    2048    |\n|      model.layers.8.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.8.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.8.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.8.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.8.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.8.mlp.up_proj.weight        |  16777216  |\n|       model.layers.8.mlp.down_proj.weight       |  16777216  |\n|      model.layers.8.input_layernorm.weight      |    2048    |\n|  model.layers.8.post_attention_layernorm.weight |    2048    |\n|      model.layers.9.self_attn.q_proj.weight     |  4194304   |\n|      model.layers.9.self_attn.k_proj.weight     |  1048576   |\n|      model.layers.9.self_attn.v_proj.weight     |  1048576   |\n|      model.layers.9.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.9.mlp.gate_proj.weight       |  16777216  |\n|        model.layers.9.mlp.up_proj.weight        |  16777216  |\n|       model.layers.9.mlp.down_proj.weight       |  16777216  |\n|      model.layers.9.input_layernorm.weight      |    2048    |\n|  model.layers.9.post_attention_layernorm.weight |    2048    |\n|     model.layers.10.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.10.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.10.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.10.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.10.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.10.mlp.up_proj.weight       |  16777216  |\n|       model.layers.10.mlp.down_proj.weight      |  16777216  |\n|      model.layers.10.input_layernorm.weight     |    2048    |\n| model.layers.10.post_attention_layernorm.weight |    2048    |\n|     model.layers.11.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.11.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.11.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.11.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.11.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.11.mlp.up_proj.weight       |  16777216  |\n|       model.layers.11.mlp.down_proj.weight      |  16777216  |\n|      model.layers.11.input_layernorm.weight     |    2048    |\n| model.layers.11.post_attention_layernorm.weight |    2048    |\n|     model.layers.12.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.12.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.12.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.12.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.12.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.12.mlp.up_proj.weight       |  16777216  |\n|       model.layers.12.mlp.down_proj.weight      |  16777216  |\n|      model.layers.12.input_layernorm.weight     |    2048    |\n| model.layers.12.post_attention_layernorm.weight |    2048    |\n|     model.layers.13.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.13.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.13.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.13.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.13.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.13.mlp.up_proj.weight       |  16777216  |\n|       model.layers.13.mlp.down_proj.weight      |  16777216  |\n|      model.layers.13.input_layernorm.weight     |    2048    |\n| model.layers.13.post_attention_layernorm.weight |    2048    |\n|     model.layers.14.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.14.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.14.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.14.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.14.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.14.mlp.up_proj.weight       |  16777216  |\n|       model.layers.14.mlp.down_proj.weight      |  16777216  |\n|      model.layers.14.input_layernorm.weight     |    2048    |\n| model.layers.14.post_attention_layernorm.weight |    2048    |\n|     model.layers.15.self_attn.q_proj.weight     |  4194304   |\n|     model.layers.15.self_attn.k_proj.weight     |  1048576   |\n|     model.layers.15.self_attn.v_proj.weight     |  1048576   |\n|     model.layers.15.self_attn.o_proj.weight     |  4194304   |\n|       model.layers.15.mlp.gate_proj.weight      |  16777216  |\n|        model.layers.15.mlp.up_proj.weight       |  16777216  |\n|       model.layers.15.mlp.down_proj.weight      |  16777216  |\n|      model.layers.15.input_layernorm.weight     |    2048    |\n| model.layers.15.post_attention_layernorm.weight |    2048    |\n|                model.norm.weight                |    2048    |\n|                   score.weight                  |    4096    |\n+-------------------------------------------------+------------+\nTotal Params: 1235818496\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"1235818496"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"## Analyze model predictions","metadata":{}},{"cell_type":"code","source":"predictions = trainer.predict(tokenized_datasets[\"validation\"])\n\nlogits = predictions.predictions\npredicted_labels = np.argmax(logits, axis=-1)  # Get the class with the highest probability\n\ntrue_labels = predictions.label_ids\n\nlabel_map = {0: \"Negative\", 1: \"Positive\"}  # Adjust according to your dataset\npredicted_classes = [label_map[label] for label in predicted_labels]\ntrue_classes = [label_map[label] for label in true_labels]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T11:57:33.112102Z","iopub.execute_input":"2024-11-20T11:57:33.112414Z","iopub.status.idle":"2024-11-20T11:58:19.877842Z","shell.execute_reply.started":"2024-11-20T11:57:33.112385Z","shell.execute_reply":"2024-11-20T11:58:19.876988Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"sentences = dataset[\"validation\"][\"sentence\"]\n\nfor i in range(20):\n    print(f\"Sentence {i+1}: {sentences[i]}\")\n    print(f\"Predicted: {predicted_classes[i]} ({predicted_labels[i]})\")\n    print(f\"Actual:    {true_classes[i]} ({true_labels[i]})\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T11:58:56.831027Z","iopub.execute_input":"2024-11-20T11:58:56.831757Z","iopub.status.idle":"2024-11-20T11:58:56.852589Z","shell.execute_reply.started":"2024-11-20T11:58:56.831720Z","shell.execute_reply":"2024-11-20T11:58:56.851784Z"}},"outputs":[{"name":"stdout","text":"Sentence 1: it 's a charming and often affecting journey . \nPredicted: Positive (1)\nActual:    Positive (1)\n\nSentence 2: unflinchingly bleak and desperate \nPredicted: Positive (1)\nActual:    Negative (0)\n\nSentence 3: allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . \nPredicted: Positive (1)\nActual:    Positive (1)\n\nSentence 4: the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales . \nPredicted: Positive (1)\nActual:    Positive (1)\n\nSentence 5: it 's slow -- very , very slow . \nPredicted: Negative (0)\nActual:    Negative (0)\n\nSentence 6: although laced with humor and a few fanciful touches , the film is a refreshingly serious look at young women . \nPredicted: Positive (1)\nActual:    Positive (1)\n\nSentence 7: a sometimes tedious film . \nPredicted: Negative (0)\nActual:    Negative (0)\n\nSentence 8: or doing last year 's taxes with your ex-wife . \nPredicted: Negative (0)\nActual:    Negative (0)\n\nSentence 9: you do n't have to know about music to appreciate the film 's easygoing blend of comedy and romance . \nPredicted: Positive (1)\nActual:    Positive (1)\n\nSentence 10: in exactly 89 minutes , most of which passed as slowly as if i 'd been sitting naked on an igloo , formula 51 sank from quirky to jerky to utter turkey . \nPredicted: Negative (0)\nActual:    Negative (0)\n\nSentence 11: the mesmerizing performances of the leads keep the film grounded and keep the audience riveted . \nPredicted: Positive (1)\nActual:    Positive (1)\n\nSentence 12: it takes a strange kind of laziness to waste the talents of robert forster , anne meara , eugene levy , and reginald veljohnson all in the same movie . \nPredicted: Positive (1)\nActual:    Negative (0)\n\nSentence 13: ... the film suffers from a lack of humor ( something needed to balance out the violence ) ... \nPredicted: Negative (0)\nActual:    Negative (0)\n\nSentence 14: we root for ( clara and paul ) , even like them , though perhaps it 's an emotion closer to pity . \nPredicted: Negative (0)\nActual:    Positive (1)\n\nSentence 15: even horror fans will most likely not find what they 're seeking with trouble every day ; the movie lacks both thrills and humor . \nPredicted: Negative (0)\nActual:    Negative (0)\n\nSentence 16: a gorgeous , high-spirited musical from india that exquisitely blends music , dance , song , and high drama . \nPredicted: Positive (1)\nActual:    Positive (1)\n\nSentence 17: the emotions are raw and will strike a nerve with anyone who 's ever had family trauma . \nPredicted: Negative (0)\nActual:    Positive (1)\n\nSentence 18: audrey tatou has a knack for picking roles that magnify her outrageous charm , and in this literate french comedy , she 's as morning-glory exuberant as she was in amélie . \nPredicted: Positive (1)\nActual:    Positive (1)\n\nSentence 19: ... the movie is just a plain old monster . \nPredicted: Negative (0)\nActual:    Negative (0)\n\nSentence 20: in its best moments , resembles a bad high school production of grease , without benefit of song . \nPredicted: Negative (0)\nActual:    Negative (0)\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"## Custom inputs","metadata":{}},{"cell_type":"code","source":"custom_dataset = [\"I love this!\", \"This was the worst experience ever.\"]\n\ncustom_tokens = tokenizer(\n    custom_dataset, \n    padding=True, \n    truncation=True, \n    return_tensors=\"pt\", \n    max_length=128\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ncustom_tokens = {key: value.to(device) for key, value in custom_tokens.items()}\n\nwith torch.no_grad():\n    custom_outputs = model(**custom_tokens)\n\ncustom_logits = custom_outputs.logits\ncustom_predicted_labels = torch.argmax(custom_logits, dim=-1).tolist()\n\nlabel_map = [\"Negative\", \"Positive\"]\ncustom_predicted_classes = [label_map[label] for label in custom_predicted_labels]\n\nfor sentence, prediction in zip(custom_dataset, custom_predicted_classes):\n    print(f\"Sentence: {sentence}\\nPrediction: {prediction}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T13:54:24.112612Z","iopub.execute_input":"2024-11-20T13:54:24.113474Z","iopub.status.idle":"2024-11-20T13:54:24.205119Z","shell.execute_reply.started":"2024-11-20T13:54:24.113426Z","shell.execute_reply":"2024-11-20T13:54:24.204265Z"}},"outputs":[{"name":"stdout","text":"Sentence: I love this!\nPrediction: Positive\n\nSentence: This was the worst experience ever.\nPrediction: Negative\n\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}